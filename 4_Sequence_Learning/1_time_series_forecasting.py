# -*- coding: utf-8 -*-
"""10_time_series_forecasting.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GueaVsSA0f9cjqKSW-AzKJzI2zeirI1u

# Time series forecasting fundamentals

# Get data

Use the Bitcoin price as the sample data. 


**Data source**: https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/BTC_USD_2013-10-01_2021-05-18-CoinDesk.csv
"""

!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/BTC_USD_2013-10-01_2021-05-18-CoinDesk.csv

"""Import wiht pandas"""

import pandas as pd
df = pd.read_csv("/content/BTC_USD_2013-10-01_2021-05-18-CoinDesk.csv",
                 parse_dates=["Date"],
                 index_col=["Date"]) # Parse the date column

df.head()

#

df.tail()

df.shape

"""Only have past 8 years's data, typically, deep learning models usually favors more data.

But smaller samples is common when coming into the time series problems.
"""

# Only want the closing price
bitcoin_prices = pd.DataFrame(df["Closing Price (USD)"]).rename({"Closing Price (USD)":"Price"},axis=1)

bitcoin_prices.head()

import matplotlib.pyplot as plt

plt.plot(bitcoin_prices["Price"])
plt.ylabel("Bitcoin price")
plt.xlabel("Date")

"""## Alternative of reading CSV

**with Python CSV module**
"""

import csv
from datetime import datetime

timesteps = []
btc_price = []
with open("/content/BTC_USD_2013-10-01_2021-05-18-CoinDesk.csv", "r") as f:
  csv_reader = csv.reader(f, delimiter=",")
  
  next(csv_reader) # Skip the header
  for line in csv_reader:
    timesteps.append(datetime.strptime(line[1], "%Y-%m-%d")) # Get the date
    btc_price.append(float(line[2])) # Get the closing price, note the `float`

timesteps[:5]

btc_price[:5]

plt.figure(figsize=(10, 7))
plt.plot(timesteps, btc_price)
plt.ylabel("Price")
plt.xlabel("Date")
plt.title("Price of Bitcoin over time")

"""## Format data part 1 - Create train and test set

## **The wrong way**⚠
"""

# Get bitcoin data array
bitcoin_prices[:5]
timesteps = bitcoin_prices.index.to_numpy()
prices = bitcoin_prices["Price"].to_numpy()

timesteps[:10], prices[:10]

# Wrong way to make train/test set for time series data
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(timesteps, prices, test_size=0.2,
                                                    random_state=42)

X_train.shape, X_test.shape, y_train.shape, y_test.shape

"""❗**The mistake is caused by the random split**, since it leads to the data leak."""

# Let's plot the wrong train and test split

plt.figure(figsize=(10,7))
plt.scatter(X_train, y_train, s=5, label="Training data")
plt.scatter(X_test, y_test, s=5, label="Test data")
plt.show()

"""Create the train/test split right for the time series data"""

split_size = int(0.8 * len(prices))

# Create train data split
X_train, y_train = timesteps[:split_size], prices[:split_size]
X_test, y_test = timesteps[split_size:], prices[split_size:]


len(X_train), len(X_test), len(y_train), len(y_test)

plt.scatter(X_train, y_train, s=4, label="Training data")
plt.scatter(X_test, y_test, s=4, label="Test data")

"""## Create a plotting function"""

# Create a plotting function

def plot_time_series(timesteps, values, format=".", start=0, end=None, label=None):
  """
  Plot time steps

  Parameters
  ----------
  timesteps: array of timestep values
  values: array of values across time
  format: style of plot, default "."
  start: where to start the plot
  end: where to end the plot
  label: label to show on plot about values, default is None
  """
  #Plot the series
  plt.plot(timesteps[start:end], values[start:end], format, label=label)
  plt.xlabel("Time")
  plt.ylabel("BTC Price")
  if label:
    plt.legend(fontsize=14) #Make the label bigger
  plt.grid(True)

# Test out the plotting function
plot_time_series(timesteps=X_train, values=y_train, label="Training data")
plot_time_series(timesteps=X_test, values=y_test, label="Training data")

"""# Modeling Experiments

Build a few models and see which performs best

- Horizon = number of timesteps into the furture we are going to predict
- Window size = number of timesteps we use to predict **Horizon**

## Model 0: Naive forecast

The formula is:

$$\hat{y}_t{t} = y_{t-1}$$

Basically, using the value at time t to predict the t+1 timestep
"""

y_test[:10]

# Create a naive forecast model
naive_forecast = y_test[:-1]

len(naive_forecast)

len(y_test)

# Plotting method
plt.figure(figsize=(10,7))
# plot_time_series(timesteps=X_train, values=y_train, label="Training data")
plot_time_series(timesteps=X_test, values=y_test, label="Test data")
plot_time_series(timesteps=X_test[1:], values=naive_forecast,format="-", label="Train data")

"""## Evaluating a time series forecasting

It is a regression problem, so a few metrics are in consideration:

- **MAE**
- **MSE**
- **RMSE**
- **MAPE/sMAPE**
- **MASE**
"""

import tensorflow as tf

# Implement the **MASE**

def mean_absolute_scaled_error(y_true, y_pred):
  """
  Implement MASE (assuming no seasonality of data)
  """
  mae = tf.reduce_mean(tf.abs(y_true-y_pred))

  # MAE of naive forecast (no seasonality)
  mae_naive_no_season = tf.reduce_mean(tf.abs(y_true[1:] - y_true[:-1]))

  return mae/mae_naive_no_season

mean_absolute_scaled_error(y_true=y_test[1:], y_pred=naive_forecast).numpy()

y_test[1:5], naive_forecast[:5]

def forecasting_metrics(y_true, y_pred):
  """
  Display all the metrics mentioned above in one function
  """
  # Make sure the data type is safe with the default of TensorFlow
  y_true = tf.cast(y_true, dtype=tf.float32)
  y_pred = tf.cast(y_pred, dtype=tf.float32)

  # MAE
  mae = tf.reduce_mean(tf.abs(y_true-y_pred))

  # MSE
  mse = tf.reduce_mean(tf.square(y_true - y_pred))


  # RMSE
  rmse = tf.sqrt(mse)

  # MAPE

  mape = tf.keras.metrics.mean_absolute_percentage_error(y_true, y_pred)

  # MASE
  mae_naive_no_season = tf.reduce_mean(tf.abs(y_true[1:] - y_true[:-1]))
  mase = mae/mae_naive_no_season

  return {"mae":mae.numpy(),
          "mse": mse.numpy(),
          "rmse": rmse.numpy(),
          "mape": mape.numpy(),
          "mase": mase.numpy()}

naive_result = forecasting_metrics(y_true = y_test[1:],
                                   y_pred = naive_forecast)

naive_result

"""# Format Data Part 2: windowing the dataset

Something like

```
[1,2,3,4,5,6] -> [7]
```
"""

len(y_train)

btc_price[:10]

# Set global variables for window and horizon size
HORIZON  = 1 # Predict next one day
WINDOW_SIZE = 7 # Using the past 7 days' data

# Create a function to label windowed data
def get_labelled_window(x, horizon=HORIZON):
  """
  Create labels for windowed dataset.
  
  E.g. if horison = 1
  Input: [0,1,2,3,4,5,6,7], Output = ([0,1,2,3,4,5,6,7], [8])
  """
  return x[:, :-horizon], x[:, -horizon:]

# Test out the labeled window function
test_window, test_labels = get_labelled_window(tf.expand_dims(tf.range(8)+1, axis=0))
test_window, test_labels

tf.expand_dims(tf.range(8)+1, axis=0)

"""## Sliding windows

There are a few options:
- **For loop** (getting slow when the dataset is large)
- **Numpy arrary indexing**

The function will:
1. Create a window step of specific window size(e.g. [1,2,3,4,5,6])
2. Use NumPy indexing to create a 2D array of multiple window steps, for example:

```
[
[1,2,3,4,5,6],
[2,3,4,5,6,7],
[3,4,5,6,7,8]
]
```

3. Use the 2D arrary of multiple window steps to index on the target variable.
4. Use the `get_labelled_window` function we created above to reshape the data
"""

import numpy as np

# Create a function to extract the moving window
def make_windows(x, window_size=WINDOW_SIZE, horizon=HORIZON):
  """
  Turns a 1D array into a 2D array of sequential labelled windows of window_size with horizon size labels.
  """
  # 1. Create a window of window_size
  window_step = np.expand_dims(np.arange(window_size+horizon), axis=0)

  # 2. Create a 2D array of multiple window steps
  window_indexes = window_step + np.expand_dims(np.arange(len(x)-(window_size+horizon-1)), axis=0).T

  print(f"Window indexes:\n {window_indexes, window_indexes.shape}")

  # 3. Index on the target array 
  windowed_array = x[window_indexes]
  print(windowed_array)

  # 4. Get the labelled window
  windows, labels = get_labelled_window(windowed_array, horizon=HORIZON)
  return windows, labels

np.expand_dims(np.arange(8),axis=0)

np.expand_dims(np.arange(8),axis=1)

np.arange(8)

full_windows, full_labels =  make_windows(prices, window_size=WINDOW_SIZE, horizon=HORIZON)
len(full_windows), len(full_labels)

# View the first 3 windows/labels
for i in range(3):
  print(f"Window: {full_windows[i]}, predicting -> label: {full_labels[i]}")

# View the last 3 windows/labels
for i in range(3):
  print(f"Window: {full_windows[i-3]}, predicting -> label: {full_labels[i-3]}")

"""## Alternative: timeseries_dataset_from_array
Alternative, **tf.keras** has a built-in function `timeseries_dataset_from_array ` to make the sliding window: https://www.tensorflow.org/api_docs/python/tf/keras/utils/timeseries_dataset_from_array 
"""

len(prices)

"""Just make samples to illustrate the indices

- input_data = [1,2,3,4,5,6,7,8,9,10,11]
- targets = [8,9,10,11,12]
"""

data = tf.range(20)
input_data = data[:-1]
targets = data[10:]
test_dataset = tf.keras.preprocessing.timeseries_dataset_from_array(
    input_data, targets, sequence_length=10)
for batch in test_dataset:
  inputs, targets = batch
  assert np.array_equal(inputs[0], data[:10])  # First sequence: steps [0-9]
  assert np.array_equal(targets[0], data[10])  # Corresponding target: step 10
  break

for batch in test_dataset.as_numpy_iterator():
  input, label  = batch
  print(f"Input:{input}, target:{label}")

test_dataset.take(1)

input_data = prices[:-HORIZON]
targets = prices[WINDOW_SIZE:]
dataset = tf.keras.preprocessing.timeseries_dataset_from_array(
    input_data, targets, sequence_length=WINDOW_SIZE)
for batch in dataset:
  inputs, targets = batch
  assert np.array_equal(inputs[0], prices[:WINDOW_SIZE])  # First sequence: steps [0-9]
  assert np.array_equal(targets[0], prices[WINDOW_SIZE])  # Corresponding target: step 10
  
  print(f"First Input:{inputs[0]}, target:{targets[0]}")
  print(f"Second Input:{inputs[1]}, target:{targets[1]}")

  break

dataset.element_spec

dataset = dataset.unbatch()

dataset.element_spec

for example_inputs, example_labels in dataset.take(1):
  print(f'Inputs shape (batch, time, features): {example_inputs.shape}')
  print(f'Labels shape (batch, time, features): {example_labels.shape}')

for input, label in dataset.take(1):
  print(input, label)

len(list(dataset))

test_split = 0.2
split_index = int(len(list(dataset)) * (1-test_split))

train_dataset = dataset.take(split_index).batch(batch_size=32)
test_dataset = dataset.skip(split_index).batch(batch_size=32)

len(list(train_dataset)), len(list(test_dataset))

train_dataset.element_spec

"""## Split data"""

def make_train_test_split(windows, labels, test_split=0.2):
  split_size = int(len(windows) * (1-test_split))
  train_window = windows[:split_size]
  train_labels = labels[:split_size]

  test_window = windows[split_size:]
  test_labels = labels[split_size:]

  return train_window, train_labels, test_window, test_labels

train_window, train_labels, test_window, test_labels = make_train_test_split(full_windows, full_labels)

"""# Make a modeling checkpoint callback

To save the best model during training.
"""

import os

def create_model_checkpoint(model_name, save_path="model_checkpoint"):
  return tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(save_path, model_name),
                                            verbose=0,
                                            save_best_only=True)

"""# Model 1: Dense Model

- hidden units = 128, relu activation
- Window Size = 7 
- Horizon = 1
"""

len(prices)

train_window.shape[0], test_window.shape[0]

import tensorflow as tf
from tensorflow.keras import layers


HORIZON  = 1 # 预测下一天的价格
WINDOW_SIZE = 7 # 基于过去7天的价格


# Set random seed

tf.random.set_seed(42)
tf.keras.backend.clear_session()

# 1. Consutruct the model

model_1 = tf.keras.models.Sequential(
[
 layers.Dense(128, activation="relu"),
 layers.Dense(HORIZON, activation="linear")
]
, name="model_1_Dense")

# 2. Compile the model
model_1.compile(loss='mae', 
                optimizer=tf.keras.optimizers.Adam(),
                metrics=['mae','mse'])

# 3. Fit the model
model_1.fit(x=train_window,
            y=train_labels,
            epochs=100,
            batch_size=128,
            verbose=0,
            validation_data=(test_window, test_labels),
            callbacks=[create_model_checkpoint(model_name=model_1.name)])

# Evaluate on the test data
model_1.evaluate(test_window, test_labels)

# Load in the best performing model
model_1 = tf.keras.models.load_model("model_checkpoint/model_1_Dense")
model_1.evaluate(test_window, test_labels)

naive_result

"""## Making forecast on the test dataset

Please note this is psudo future forecasting
"""

def make_preds(model, input_data):
  """
  Uses a model to predict on the input data.
  """
  forecast = model.predict(input_data)
  return tf.squeeze(forecast) # Return 1D dataset

model_1_preds = make_preds(model_1, test_window)
len(model_1_preds), model_1_preds[:10]

model_1_preds.shape

# Evaluate preds
model_1_results = forecasting_metrics(y_true=tf.squeeze(test_labels), y_pred=model_1_preds)
model_1_results

# Plot the model 1 prediction
offset = 400
plt.figure(figsize=(10,7))
plot_time_series(timesteps=X_test[-len(test_window):],
                 values=test_labels[:],
                 start=offset,
                 label="Test data"
                 )
plot_time_series(timesteps=X_test[-len(test_window):],
                 values=model_1_preds[:],
                 start=offset,
                 format="-",
                 label="Model 1 preds"
                 )

"""## Model 1 alternative

Using the dataset prepared by the **tf.keras** utils for time series, with the same architecture though.
"""

tf.random.set_seed(42)
tf.keras.backend.clear_session()

model_1_altnv = tf.keras.models.Sequential(
[
 layers.Input(WINDOW_SIZE),
 layers.Dense(128, activation="relu"),
 layers.Dense(HORIZON, activation="linear")
]
, name="model_1_Dense_altnv")

# 2. Compile the model
model_1_altnv.compile(loss='mae', 
                optimizer=tf.keras.optimizers.Adam(),
                metrics=['mae'])

# 3. Fit the model
model_1_altnv.fit( train_dataset,
            epochs=100,
            batch_size=128,
            verbose=0,
            validation_data=test_dataset,
            callbacks=[create_model_checkpoint(model_name=model_1_altnv.name)]
            )

model_1_altnv.evaluate(test_dataset)

model_1_altnv = tf.keras.models.load_model("model_checkpoint/model_1_Dense_altnv")

model_1_altnv.evaluate(test_dataset)

model_1_altnv_preds = model_1_altnv.predict(test_dataset)
model_1_altnv_preds.shape

model_1_altnv_train_preds = model_1_altnv.predict(train_dataset)
model_1_altnv_train_preds.shape

# Evaluate preds
model_1_altnv_results = forecasting_metrics(y_true=tf.squeeze(test_labels), y_pred=model_1_altnv_train_preds)
model_1_altnv_results

test_price = test_dataset.unbatch()
for input, label in test_price.take(1):
  print(label)

test_price = test_price.map(lambda input, label: label)
trst_prices = []
for price in  test_price.as_numpy_iterator():
  trst_prices.append(price)

len(timesteps[split_index:])

len(prices)

# Plot the model 1 prediction
offset = 400
plt.figure(figsize=(10,7))
plot_time_series(timesteps=X_test[-len(test_window):],
                 values=trst_prices[:],
                 start=offset,
                 label="Test data"
                 )
plot_time_series(timesteps=X_test[-len(test_window):],
                 values=model_1_altnv_preds[:],
                 start=offset,
                 format="-",
                 label="Model 1 alternative preds"
                 )

"""# Model 2: Dense(window=30, horizon=1)

"""

import tensorflow as tf

HORIZON = 1
WINDOW_SIZE = 30

full_windows, full_labels = make_windows(prices,
                                         window_size=WINDOW_SIZE,
                                         horizon=HORIZON)
len(full_windows), len(full_labels)

# Make train and testing windows
train_windows, train_labels, test_windows, test_labels = make_train_test_split(full_windows, full_labels)

len(train_windows), len(test_windows), len(train_labels), len(test_labels)

train_windows[:3]

# Set random seed

tf.random.set_seed(42)
tf.keras.backend.clear_session()

# 1. Consutruct the model

model_2 = tf.keras.models.Sequential(
[
 layers.Dense(128, activation="relu"),
 layers.Dense(HORIZON, activation="linear")
]
, name="model_2_Dense")

# 2. Compile the model
model_2.compile(loss='mae', 
                optimizer=tf.keras.optimizers.Adam())

# 3. Fit the model
model_2.fit(x=train_windows,
            y=train_labels,
            epochs=100,
            batch_size=128,
            verbose=0,
            validation_data=(test_windows, test_labels),
            callbacks=[create_model_checkpoint(model_name=model_2.name)]
            )

#Evaluate the model 2
model_2.evaluate(test_windows, test_labels)

model_2 = tf.keras.models.load_model("/content/model_checkpoint/model_2_Dense")
model_2.evaluate(test_windows, test_labels)

def evaluate_preds(y_true, y_pred):
  """
  Display all the metrics mentioned above in one function
  """
  # Make sure the data type is safe with the default of TensorFlow
  y_true = tf.cast(y_true, dtype=tf.float32)
  y_pred = tf.cast(y_pred, dtype=tf.float32)

  mae = tf.reduce_mean(tf.abs(y_true-y_pred))
  mse = tf.reduce_mean(tf.square(y_true - y_pred))
  rmse = tf.sqrt(mse)
  mape = tf.keras.metrics.mean_absolute_percentage_error(y_true, y_pred)

  mae_naive_no_season = tf.reduce_mean(tf.abs(y_true[1:] - y_true[:-1]))
  mase = mae/mae_naive_no_season

  # Account for different sized metrics because of longer horizon
  if mae.ndim > 0:
    mae = tf.reduce_mean(mae)
    mse = tf.reduce_mean(mse)
    rmse = tf.reduce_mean(rmse)
    mape = tf.reduce_mean(mape)
    mase = tf.reduce_mean(mase)

  return {"mae":mae.numpy(),
          "mse": mse.numpy(),
          "rmse": rmse.numpy(),
          "mape": mape.numpy(),
          "mase": mase.numpy()
          }

# Get forecast predictions
model_2_preds = make_preds(model_2, input_data=test_windows)
model_2_results = evaluate_preds(y_true=tf.squeeze(test_labels), y_pred=model_2_preds)
model_2_results

offset = 300
plt.figure(figsize=(10,7))
# Account for offset when plotting the predictions
plot_time_series(timesteps=X_test[-len(test_windows):],
                 values=test_labels,
                 start=offset,
                 label="Test data")

plot_time_series(timesteps=X_test[-len(test_windows):],
                 values=model_2_preds,
                 start=offset,
                 label="Model 2 predicitons")

"""## Model 3: Dense(window=30, horizon=7)"""

HORIZON = 7
WINDOW_SIZE = 30

full_windows, full_labels = make_windows(prices,
                                         window_size=WINDOW_SIZE,
                                         horizon=HORIZON)
len(full_windows), len(full_labels)

# Make train and testing windows
train_windows, train_labels, test_windows, test_labels = make_train_test_split(full_windows, full_labels)

len(train_windows), len(test_windows), len(train_labels), len(test_labels)

test_windows.shape, test_labels.shape

# Modeling

# Set random seed

tf.random.set_seed(42)
tf.keras.backend.clear_session()

# 1. Consutruct the model

model_3 = tf.keras.models.Sequential(
[
 layers.Dense(128, activation="relu"),
 layers.Dense(HORIZON, activation="linear")
]
, name="model_3_Dense")

# 2. Compile the model
model_3.compile(loss='mae', 
                optimizer=tf.keras.optimizers.Adam())

# 3. Fit the model
model_3.fit(x=train_windows,
            y=train_labels,
            epochs=100,
            batch_size=128,
            verbose=0,
            validation_data=(test_windows, test_labels),
            callbacks=[create_model_checkpoint(model_name=model_3.name)]
            )

#Evaluate the model 3
model_3.evaluate(test_windows, test_labels)

model_3 = tf.keras.models.load_model("/content/model_checkpoint/model_3_Dense")
model_3.evaluate(test_windows, test_labels)

# Get forecast predictions
model_3_preds = make_preds(model_3, input_data=test_windows)

"""## Making the evaluation function work for multi-dim"""



# Get forecast predictions
model_3_preds = make_preds(model_3, input_data=test_windows)
model_3_results = evaluate_preds(y_true=tf.squeeze(test_labels), y_pred=model_3_preds)
model_3_results

offset = 300
plt.figure(figsize=(10,7))
# Account for offset when plotting the predictions
plot_time_series(timesteps=X_test[-len(test_windows):],
                 values=test_labels[:,0],
                 start=offset,
                 label="Test data")

plot_time_series(timesteps=X_test[-len(test_windows):],
                 values=tf.reduce_mean(model_3_preds, axis=1),
                 start=offset,
                 format="-",
                 label="Model 3 predicitons")

"""# Model 4: Conv1D"""

HORIZON = 1
WINDOW_SIZE = 7

full_windows, full_labels = make_windows(prices, window_size=WINDOW_SIZE, horizon=HORIZON)
len(full_windows), len(full_labels)

# Make train and testing windows
train_windows, train_labels, test_windows, test_labels = make_train_test_split(full_windows, full_labels)

len(train_windows), len(test_windows), len(train_labels), len(test_labels)

"""We need an input shape of: `（batch_size, timesteps, input_dim`"""

train_window[0].shape

# Need to reshape the tensor before feeding it into the Convoluation Neural Net
x = tf.constant(train_windows[0])
x

expand_dim_layer = layers.Lambda(lambda x: tf.expand_dims(x, axis=1))  # Add an extra dimension for the input

print(f'Original shape: {x.shape}, \n expanded shape: {expand_dim_layer(x).shape}, \n Original values after expansion: {expand_dim_layer(x)}')

# Modeling

# Set random seed

tf.random.set_seed(42)
tf.keras.backend.clear_session()

# 1. Consutruct the model

model_4 = tf.keras.models.Sequential(
[
 expand_dim_layer,
 layers.Conv1D(filters=128, kernel_size=5, padding="causal", activation="relu"),
 layers.Dense(HORIZON, activation="linear")
]
, name="model_4_Conv1D")

# 2. Compile the model
model_4.compile(loss='mae', 
                optimizer=tf.keras.optimizers.Adam())

# 3. Fit the model
model_4.fit(x=train_windows,
            y=train_labels,
            epochs=100,
            batch_size=128,
            verbose=0,
            validation_data=(test_windows, test_labels),
            callbacks=[create_model_checkpoint(model_name=model_4.name)]
            )

model_4.summary()

# Model evaluation
model_4.evaluate(test_windows, test_labels)

model_4 = tf.keras.models.load_model("model_checkpoint/model_4_Conv1D")
model_4.evaluate(test_windows, test_labels)

# Get forecast predictions
model_4_preds = make_preds(model_4, input_data=test_windows)
model_4_results = evaluate_preds(y_true=tf.squeeze(test_labels), y_pred=model_4_preds)
model_4_results

offset = 300
plt.figure(figsize=(10,7))
# Account for offset when plotting the predictions
plot_time_series(timesteps=X_test[-len(test_windows):],
                 values=test_labels[:,0],
                 start=offset,
                 label="Test data")

plot_time_series(timesteps=X_test[-len(test_windows):],
                 values= model_4_preds ,
                 start=offset,
                 format="-",
                 label="Model 4 predicitons")

"""# Model 5 LSTM"""

# Modeling

# Set random seed

tf.random.set_seed(42)
tf.keras.backend.clear_session()

# 1. Consutruct the model

model_5 = tf.keras.models.Sequential(
[
 expand_dim_layer,
 layers.LSTM(128, activation="relu"),
 layers.Dense(HORIZON, activation="linear")
]
, name="model_5_LSTM")

# 2. Compile the model
model_5.compile(loss='mae', 
                optimizer=tf.keras.optimizers.Adam())

# 3. Fit the model
model_5.fit(x=train_windows,
            y=train_labels,
            epochs=100,
            batch_size=128,
            verbose=0,
            validation_data=(test_windows, test_labels),
            callbacks=[create_model_checkpoint(model_name=model_5.name)]
            )

model_5.evaluate(test_windows, test_labels)

model_5 = tf.keras.models.load_model("/content/model_checkpoint/model_5_LSTM")
model_5.evaluate(test_windows, test_labels)

# Get forecast predictions
model_5_preds = make_preds(model_5, input_data=test_windows)
model_5_results = evaluate_preds(y_true=tf.squeeze(test_labels), y_pred=model_5_preds)
model_5_results

"""# Multvariate forcasting"""

bitcoin_prices.sort_values(by="Date").head()

bitcoin_prices.sort_values(by="Date").tail()

# Add a bitcoin  halving events to our datasets

block_reward_1 = 50 # 3 Jan 2009
block_reward_2 = 25 # 28 Nov 2012
block_reward_3 = 12.5 # 9 July 2016
block_reward_4 = 6.25 # 18 May 2020

# Block reward dates
# block_reward_1_datetime = bitcoin_prices.index[0]
block_reward_2_datetime = bitcoin_prices.index[0]

# block_reward_2_datetime = np.datetime64("2012-11-28")
block_reward_3_datetime = np.datetime64("2016-07-09")
block_reward_4_datetime = np.datetime64("2020-05-11")

block_reward_df = pd.DataFrame({"Date":[
                                        block_reward_2_datetime,
                                        block_reward_3_datetime,
                                        block_reward_4_datetime],
                                "Rewards":[ 
                                           block_reward_2,
                                           block_reward_3,
                                           block_reward_4]})

block_reward_df.sort_values(by="Date")

# Create date ranges of where specific block_reward values should be
bitcoin_prices_block = bitcoin_prices.copy()

bitcoin_prices_block = bitcoin_prices_block.merge(block_reward_df, on="Date", how="left")

bitcoin_prices_block[bitcoin_prices_block["Rewards"]>0]

# Fowward fill the NaN values 
bitcoin_prices_block.ffill(axis = 0 , inplace=True)

bitcoin_prices_block

# Plot the block reward vs. price over time
from sklearn.preprocessing import minmax_scale
scaled_price_block_df = pd.DataFrame(minmax_scale(bitcoin_prices_block[["Price","Rewards"]]),
                                     columns= ["Price","Rewards"],
                                     index=bitcoin_prices_block.Date
                                                                                                     
                                    )
scaled_price_block_df.plot(figsize=(10,7))

"""## Make a window function for multi-variate with Pandas"""

# Set up dataset hyper params
HORIZON = 1
WINDOW_SIZE = 7

# Make a copy of Bitcoin historical data with block reward
bitcoin_prices_windowed = bitcoin_prices_block.copy()

for i in range(WINDOW_SIZE): 
  bitcoin_prices_windowed[f"Price+{i+1}"] = bitcoin_prices_windowed["Price"].shift(periods=i+1)

bitcoin_prices_windowed.head(10)

"""The data becomes as below:

```
[0,1,2,3,4,5,6, block_reward] -> [7]
[1,2,3,4,5,6,7, block_reward] -> [8]

```
"""

bitcoin_prices_windowed.set_index("Date",inplace=True)
X = bitcoin_prices_windowed.dropna().drop("Price", axis=1).astype(np.float32)
y = bitcoin_prices_windowed.dropna()["Price"].astype(np.float32)

X.head()

# Make train and test set with indexing
split_size = int(len(X) * 0.8)
X_train, y_train = X[:split_size], y[:split_size]
X_test, y_test = X[split_size:], y[split_size:]

len(X_train), len(X_test), len(y_train), len(y_test)

"""# Model 6: Dense(multi-variate time series)"""

tf.random.set_seed(42)

tf.keras.backend.clear_session()

model_6 = tf.keras.models.Sequential([
  layers.Dense(128, activation="relu"),
  layers.Dense(128, activation="relu"),
  layers.Dense(HORIZON)
], name="model_6_dense_mutlivariate")

model_6.compile(loss="mae",
                optimizer=tf.keras.optimizers.Adam())

model_6.fit(X_train, y_train,
            epochs=100,
            batch_size=128,
            verbose=0,
            validation_data=(X_test, y_test),
            callbacks=[create_model_checkpoint(model_name=model_6.name)]
            )

model_6.evaluate(X_test, y_test)

model_6 = tf.keras.models.load_model("model_checkpoint/model_6_dense_mutlivariate")
model_6.evaluate(X_test, y_test)

# Get forecast predictions
model_6_preds = make_preds(model_6, input_data=X_test)
model_6_results = evaluate_preds(y_true=y_test, y_pred=model_6_preds)
model_6_results

naive_result

"""## Model 7: N-BEATS Algorithm

- Practice to replicating papers with TensorFlow
- Tensorflow subclassing to build customized models
- Get exp with Functional API

## Building and testing the N-BEATS block layer
"""

class NBeatsBlock(tf.keras.layers.Layer):
  def __init__(self,
               input_size:int,
               theta_size:int,
               horizon:int,
               n_neurons:int,
               n_layers:int,
               **kwargs): # The **kwargs take care of the arguments in the parent class
    super().__init__(**kwargs)
    self.input_size = input_size
    self.theta_size = theta_size
    self.horizon = horizon
    self.n_neurons = n_neurons
    self.n_layers = n_layers

    # Block contains stack of 4 fully connected layers and each has ReLU activation
    self.hidden = [tf.keras.layers.Dense(n_neurons, activation="relu") for _ in range(n_layers)]
    # Output of block is a theta layer with linear activation
    self.theta_layers = tf.keras.layers.Dense(theta_size, activation="linear", name="Theta")
  
  def call(self, inputs):
    x = inputs
    for layers in self.hidden:
      x = layers(x)
    theta = self.theta_layers(x)
    # Output the backcast and forecast from theta
    backcast, forecast = theta[:, :self.input_size], theta[:, -self.horizon:]
    return backcast, forecast

"""Test the N-BEATS class.

Firstly, use some dummy input and output.
"""

dummy_nbeats_block_layer = NBeatsBlock(input_size=WINDOW_SIZE,
                                       theta_size=WINDOW_SIZE+HORIZON, # backcast + forecast
                                       horizon=HORIZON,
                                       n_neurons=128,
                                       n_layers=4)

dummy_input = tf.expand_dims(tf.range(WINDOW_SIZE)+1, axis=0)
backcast, forecast = dummy_nbeats_block_layer(dummy_input)
backcast, forecast

"""# Prepare data for the N-BEATS model"""

HORIZON = 1
WINDOW_SIZE = 7

bitcoin_prices.head()

bitcoin_prices_nbeats = bitcoin_prices.copy()
for i in range(WINDOW_SIZE):
  bitcoin_prices_nbeats[f"Price+{i+1}"] = bitcoin_prices_nbeats["Price"].shift(periods=i+1)

bitcoin_prices_nbeats.head(10)

# Make features and labels
X = bitcoin_prices_nbeats.dropna().drop("Price", axis=1)
y = bitcoin_prices_nbeats.dropna()["Price"]

# Make train and test sets
split_size = int(len(X)*0.8)
X_train, y_train = X[:split_size], y[:split_size]
X_test, y_test = X[split_size:], y[split_size:]

len(X_test), len(y_test)

train_features_dataset = tf.data.Dataset.from_tensor_slices(X_train)
train_labels_dataset = tf.data.Dataset.from_tensor_slices(y_train)

test_features_dataset = tf.data.Dataset.from_tensor_slices(X_test)
test_labels_dataset = tf.data.Dataset.from_tensor_slices(y_test)

# Combine labels and features by zipping together -> (features, labels)

train_dataset = tf.data.Dataset.zip((train_features_dataset, train_labels_dataset))
test_dataset = tf.data.Dataset.zip((test_features_dataset, test_labels_dataset))

# Batch and prefetch
BATCH_SIZE = 1024
train_dataset = train_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)
test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)

train_dataset, test_dataset

"""### Setting up hyper params for N-BEATS model"""

# Values from the N-BEATS paper
N_EPOCHS = 5000
N_NEURONS = 512
N_LAYERS = 4
N_STACKS = 30

INPUT_SIZE = WINDOW_SIZE * HORIZON
THETA_SIZE = INPUT_SIZE + HORIZON

INPUT_SIZE, THETA_SIZE

"""### Residual connection"""

# Make tensors
tensor_1 = tf.range(10) + 10
tensor_2 = tf.range(10)

# subtract
subtracted = layers.subtract([tensor_1, tensor_2])

# Add 
added = layers.add([tensor_1,tensor_2])

print(f"Input tensors: {tensor_1.numpy()} & {tensor_2.numpy()}")
print(f"Subtracted tensors: {subtracted}")
print(f"Added tensors: {added}")

"""### Train the N-BEATS model

1. Set up an instance of the N-BEAT block
2. Create an input layer for the N-BEATS stack
3. Make the initial backcast and forecast for the model wiht the layer created in (1)
4. Use for loop to create stacks of block layers
5. Use the N-BeatsBlock class within the for loop to create blocks which return backforest and forecast 
6. Create the model double residule stacking using subtract and add layers
7. Put the model inputs and outputs using `tf.keras.model`

"""

# Create the model using Functional API
tf.random.set_seed(42)

tf.keras.backend.clear_session()


# 1. Build the model

nbeats_block_layer = NBeatsBlock(input_size=WINDOW_SIZE,
                                       theta_size=WINDOW_SIZE+HORIZON, # backcast + forecast
                                       horizon=HORIZON,
                                       n_neurons=N_NEURONS,
                                       n_layers=N_LAYERS)

inputs = tf.keras.layers.Input(INPUT_SIZE)
backcast, forecast = nbeats_block_layer(inputs)

stacked_inputs = tf.keras.layers.Subtract()([inputs, backcast])
 
for i, _ in enumerate(range(N_STACKS-1)):
  nbeats_block_layer_mdl = NBeatsBlock(input_size=WINDOW_SIZE,
                                       theta_size=WINDOW_SIZE+HORIZON, # backcast + forecast
                                       horizon=HORIZON,
                                       n_neurons=N_NEURONS,
                                       n_layers=N_LAYERS,
                                   name=f"N_BEATS_{i}")
  backcast, forecast_mdl = nbeats_block_layer_mdl(stacked_inputs)
  stacked_inputs = tf.keras.layers.Subtract()([stacked_inputs, backcast])
  forecast = tf.keras.layers.Add()([forecast_mdl, forecast])

model_7 = tf.keras.models.Model(inputs=inputs, outputs=forecast, name="model_7_nbeats")


# 2. Compile the model
model_7.compile(loss="mae",
                optimizer=tf.keras.optimizers.Adam(),
                metrics="mae")

# Model summary
model_7.summary()

# Callbacks
early_stopping = tf.keras.callbacks.EarlyStopping(patience=100)

# 3. Fit the model
model_7.fit(train_dataset,
            epochs=N_EPOCHS,
            verbose=0,
            validation_data=test_dataset,
            callbacks=[early_stopping, 
                       create_model_checkpoint(model_name=model_7.name) ,
                       tf.keras.callbacks.ReduceLROnPlateau("val_loss", patience=100)])

model_7.evaluate(test_dataset)

model_7 = tf.keras.models.load_model("model_checkpoint/model_7_nbeats")
model_7.evaluate(test_dataset)

model_7_preds = model_7.predict(test_dataset)

model_7_preds.shape

model_7_results = evaluate_preds(y_true=y_test,
                                 y_pred=tf.squeeze(model_7_preds))
model_7_results

"""### Plot the model"""

from tensorflow.keras.utils import plot_model
plot_model(model_7)

"""# Model 8: Ensamble model"""

def get_ensemble_models(horizon=HORIZON,
                        train_data=train_dataset,
                        test_data=test_dataset,
                        num_iter=10,
                        num_epochs=100,
                        loss_fns=["mae","mse","mape"]):
  
  # Make empty list for trained ensemble models
  ensemble_model = []

  for i in range(num_iter):
    for loss_function in loss_fns:
      print(f"Optimizing model by reducing {loss_function} for {num_epochs} epochs, model number: {i}" )

      model = tf.keras.Sequential([
      # Initialize dense layers with normal distribution for the estimation prediciton intervals later on
        layers.Dense(128, kernel_initializer="he_normal", activation="relu"),
        layers.Dense(128, kernel_initializer="he_normal", activation="relu"),
        layers.Dense(horizon)
      ])

      # Compile simple model with current loss function
      model.compile(loss=loss_function,
                    optimizer=tf.keras.optimizers.Adam(),
                    metrics=["mae","mse"])
      
      # Fit the current model
      model.fit(train_data,
                epochs=num_epochs,
                validation_data=test_data,
                verbose=0,
                callbacks=[tf.keras.callbacks.EarlyStopping(monitor="val_loss",
                                                            patience=200,
                                                            restore_best_weights=True),
                          tf.keras.callbacks.ReduceLROnPlateau(monitor="val_loss",
                                                               patience=200,
                                                               verbose=1)
                          ]
                            )
      ensemble_model.append(model)
  return ensemble_model

# Commented out IPython magic to ensure Python compatibility.
# %%time
# ensamble_model = get_ensemble_models(num_iter=5,
#                                      num_epochs=1000)

def make_ensamble_preds(ensamble_models, data):
  ensamble_preds = []
  for model in ensamble_models:
    preds = model.predict(data)
    ensamble_preds.append(preds)
  return tf.constant(tf.squeeze(ensamble_preds))

# Commented out IPython magic to ensure Python compatibility.
# %%time
# ensamble_preds = make_ensamble_preds(ensamble_models=ensamble_model,
#                                      data=test_dataset)
# ensamble_preds

ensamble_preds.shape

ensamble_results = evaluate_preds(y_true=y_test,
                                  y_pred=ensamble_preds)
ensamble_results

ensamble_preds_mean = tf.reduce_mean(ensamble_preds, axis=0)
ensamble_preds_mean.shape

ensamble_preds_median = np.median(ensamble_preds, axis=0)
ensamble_preds_median.shape

ensamble_results = evaluate_preds(y_true=y_test,
                                  y_pred=ensamble_preds_mean)
ensamble_results

ensamble_results = evaluate_preds(y_true=y_test,
                                  y_pred=ensamble_preds_median)
ensamble_results

"""## Prediction intervals （uncertainty estimates)"""

# Prediction based on the predictions of ensamble models
def get_upper_lower(preds): 
  # standard deviation
  std = tf.math.reduce_std(preds, axis=0)

  interval = 1.96 * std

  preds_mean = tf.reduce_mean(preds, axis=0)
  lower, upper = preds_mean - interval, preds_mean + interval
  return lower, upper

lower, upper = get_upper_lower(ensamble_preds)

ensamble_median = np.median(ensamble_preds, axis=0)

# Plot the median of ensamble preds
offset=500
plt.figure(figsize=(10,7))
plt.plot(X_test.index[offset:], y_test[offset:], "o", label="Test data")
plt.plot(X_test.index[offset:], ensamble_median[offset:], "-", label="Ensamble Median")
plt.xlabel("Date")
plt.ylabel("Bitcoin Price")
plt.fill_between(X_test.index[offset:], 
                 (lower)[offset:],
                 (upper)[offset:], label="Prediction intervals")
plt.legend()

"""## Aside: two types of uncertainty(coconut and subway)

Uncertainty estimates in ML seeks out to qualitively and quantitively answer the questions:

- What can my model know?
- What doesn't my model know?

# Model 9: predict the future

Just use the entire dataset for training. Otherwise tehy are mostly the same.
"""

HORIZON = 1
WINDOW_SIZE = 7

bitcoin_prices_windowed.head()

# Train model on entire data to make prediction for the next day 
X_all = bitcoin_prices_windowed.drop(["Price", "Rewards"], axis=1).dropna().to_numpy() # only want prices, our future model can be a univariate model
y_all = bitcoin_prices_windowed.dropna()["Price"].to_numpy()

# 1. Turn X and y into tensor Datasets
features_dataset_all = tf.data.Dataset.from_tensor_slices(X_all)
labels_dataset_all = tf.data.Dataset.from_tensor_slices(y_all)

# 2. Combine features & labels
dataset_all = tf.data.Dataset.zip((features_dataset_all, labels_dataset_all))

# 3. Batch and prefetch for optimal performance
BATCH_SIZE = 1024 # taken from Appendix D in N-BEATS paper
dataset_all = dataset_all.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)

dataset_all

# 1. Turn X and y into tensor Datasets
features_dataset_all = tf.data.Dataset.from_tensor_slices(X_all)
labels_dataset_all = tf.data.Dataset.from_tensor_slices(y_all)

# 2. Combine features & labels
dataset_all = tf.data.Dataset.zip((features_dataset_all, labels_dataset_all))

# 3. Batch and prefetch for optimal performance
BATCH_SIZE = 1024 # taken from Appendix D in N-BEATS paper
dataset_all = dataset_all.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)

dataset_all

tf.random.set_seed(42)

# Create model (nice and simple, just to test)
model_9 = tf.keras.Sequential([
  layers.Dense(128, activation="relu"),
  layers.Dense(128, activation="relu"),
  layers.Dense(HORIZON)
])

# Compile
model_9.compile(loss=tf.keras.losses.mae,
                optimizer=tf.keras.optimizers.Adam())

# Fit model on all of the data to make future forecasts
model_9.fit(dataset_all,
            epochs=100,
            verbose=0) # don't print out anything, we've seen this all before

"""## Make a function to predict in the future"""

INTO_FUTURE = 14

def predict_future(values, model, into_future, window_size=WINDOW_SIZE):
  future_preds = []
  last_window = values[-window_size:]
  # Make prediction future_steps times
  for i in range(into_future):
    pred = model.predict(tf.expand_dims(last_window, axis=0))
    
    print(f"Last window:{last_window}, -> Prediction: {pred}\n")

    future_preds.append(tf.squeeze(pred).numpy())
    last_window = np.append(last_window, pred)[-window_size:]
  
  return future_preds

# Make forecasts into the future
future_forecast = predict_future(values=y_all,
                                 model=model_9,
                                 into_future=INTO_FUTURE,
                                 window_size=WINDOW_SIZE)

"""## Plot the future prediction"""

future_forecast

def get_future_dates(start_date, into_future, offset=1):

  start_date = start_date + np.timedelta64(offset,"D")
  end_date = start_date + np.timedelta64(into_future, "D")
  return np.arange(start_date, end_date, dtype="datetime64[D]")

last_timestep = bitcoin_prices.index[-1]
last_timestep

next_time_steps = get_future_dates(last_timestep, INTO_FUTURE, offset=1)
next_time_steps

next_time_steps = np.insert(next_time_steps, 0, last_timestep)
future_forecast = np.insert(future_forecast, 0, bitcoin_prices.iloc[-1])

# Plot future prices predictions
plt.figure(figsize=(10,7))
plot_time_series(bitcoin_prices.index, bitcoin_prices, start=2500, format="-", label="Actual BTC Price")
plot_time_series(next_time_steps, future_forecast,   format="-", label="Predicted BTC Price")

