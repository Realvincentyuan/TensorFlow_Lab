# -*- coding: utf-8 -*-
"""02_classification_with_neural_network.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ze0aTcvUBdK9uueUCLBupIvmh377BUTP

# Create data to view and fit
"""

from sklearn.datasets import make_circles
import tensorflow as tf

# number of samples

n_samples = 1000

# Create circles

X, y = make_circles(n_samples,
                    noise=0.2,
                    random_state=42
                    )

X.shape

X.dtype

X_tensor = tf.constant(X)

X_tensor = tf.cast(X_tensor, dtype=tf.float32)

y_tensor = tf.constant(y)

y_tensor = tf.cast(y_tensor, dtype=tf.float32)

X_tensor.dtype

import pandas as pd

df = pd.DataFrame({"X0":X[:,0], "X1":X[:,1], "Y":y})

df.head()

from sklearn.model_selection import train_test_split

"""# Modeling for classification"""

import tensorflow as tf

tf.keras.backend.clear_session()

# Set the random seed
tf.random.set_seed(42)

# 1. Create the model using sequential API

model = tf.keras.models.Sequential([
    tf.keras.layers.Input(shape=2),
    tf.keras.layers.Dense(1)                              
])

# 2. Compile the model

model.compile(loss=tf.keras.losses.binary_crossentropy,
              optimizer=tf.keras.optimizers.SGD(),
              metrics=tf.keras.metrics.Accuracy()
              )

model.summary()

# 3. Model fit
epochs = 5
model.fit(X_tensor, y_tensor, epochs=epochs)

"""# Improve the model"""

tf.keras.backend.clear_session()

# Set the random seed
tf.random.set_seed(42)

# 1. Create the model using sequential API

model = tf.keras.models.Sequential([
    tf.keras.layers.Input(shape=2),
    tf.keras.layers.Dense(64, activation="relu"),
    tf.keras.layers.Dense(128, activation="relu"),    
    tf.keras.layers.Dense(1)
])

# 2. Compile the model

model.compile(loss=tf.keras.losses.binary_crossentropy,
              optimizer=tf.keras.optimizers.SGD(momentum=0.9),
              metrics='accuracy'
              )

model.summary()

# 3. Model fit
epochs = 50
model.fit(X_tensor, y_tensor, epochs=epochs)

model.evaluate(X_tensor, y_tensor)

y_pred = model.predict(X_tensor)

"""# Create a function to visualize the predictions

- Using numpy meshgrid
"""

import numpy as np
import matplotlib.pyplot as plt 

def plot_decision_boundary(model, X, y):
  """
  
  """
  x_min, x_max =  X[:, 0].min() - 0.1, X[:, 0].max() + 0.1
  y_min, y_max =  X[:, 1].min() - 0.1, X[:, 1].max() + 0.1
  xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),
                       np.linspace(y_min, y_max, 100),
                       )
  # Create X value
  x_in = np.c_[xx.ravel(), yy.ravel()] # stack 2D arrays together

  # Make predictions
  y_pred = model.predict(x_in)

  if len(y_pred[0]) >1:
    print("doing multi-class classfication")
    y_pred = np.argmax(y_pred, axis=1).reshape(xx.shape)
  else:
    print("doing binary classification")
    y_pred = np.round(y_pred).reshape(xx.shape)

  # plot the decision boundary
  plt.contourf(xx, yy, y_pred, cmap=plt.cm.RdYlBu, alpha=0.7)
  plt.scatter(X[:,0],X[:,1], c=y, s=40, cmap=plt.cm.RdYlBu)
  plt.xlim(xx.min(), xx.max())
  plt.ylim(yy.min(), yy.max())

plot_decision_boundary(model, X, y)

"""# Add the non-linerity of the model"""

# set the random seed
tf.random.set_seed(42)

# 1. create the model

model = tf.keras.models.Sequential([
  tf.keras.layers.Dense(1, activation=tf.keras.activations.relu)                                  

])

# 2. compile the model

model.compile(loss='binary_crossentropy',
              optimizer=tf.keras.optimizers.Adam(lr=0.001),
              metrics=['binary_crossentropy'])

# 3. fit the moddel
history = model.fit(X,y, epochs=100)

# check the input data 
plt.scatter(X[:,0],X[:,1],y)

plot_decision_boundary(model, X, y)

# create non-linearity with more hidden layers

# set the random seed
tf.random.set_seed(42)

# 1. create the model

model = tf.keras.models.Sequential([
  tf.keras.layers.Dense(100, activation=tf.keras.activations.relu) ,
  tf.keras.layers.Dense(100, activation=tf.keras.activations.relu) ,                                                                 
  tf.keras.layers.Dense(1, activation=tf.keras.activations.sigmoid)                                  
])

# 2. compile the model

model.compile(loss='binary_crossentropy',
              optimizer=tf.keras.optimizers.Adam(lr=0.001),
              metrics=['binary_crossentropy'])

# 3. fit the moddel
history = model.fit(X,y, epochs=100)

plot_decision_boundary(model, X, y)

len(X)

history.history.keys()

"""## Find the right learning rate

There are a few ways:

- A learning rate callback
- Another model
- A modified loss curve
"""

# create non-linearity with more hidden layers

# set the random seed
tf.random.set_seed(42)

# 1. create the model

model = tf.keras.models.Sequential([
  tf.keras.layers.Dense(100, activation=tf.keras.activations.relu) ,
  tf.keras.layers.Dense(100, activation=tf.keras.activations.relu) ,                                                                 
  tf.keras.layers.Dense(1, activation=tf.keras.activations.sigmoid)                                  
])

# 2. compile the model

model.compile(loss='binary_crossentropy',
              optimizer=tf.keras.optimizers.Adam(lr=0.001),
              metrics=['binary_crossentropy'])

# 3. fit the moddel
def scheduler(epoch, lr):
   if epoch < 10:
     return lr
   else:
     return lr * tf.math.exp(-0.1)

lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-4 * 10 **(epoch/20))

history = model.fit(X,y, epochs=100, callbacks=[lr_scheduler])

history = pd.DataFrame(history.history)

history.plot(figsize=(10,7), xlabel="epochs")

history.head()

plt.scatter(range(len(history)),history['lr'])

# Plot the learning rate versus the loss
lrs = 1e-4 * (10 ** (np.arange(100)/20))
plt.figure(figsize=(10, 7))
plt.semilogx(lrs, history["loss"]) # we want the x-axis (learning rate) to be log scale
plt.xlabel("Learning Rate")
plt.ylabel("Loss")
plt.title("Learning rate vs. loss");

"""## Classification metrics"""

# Split data into train and test sets
X_train, y_train = X[:800], y[:800] # 80% of the data for the training set
X_test, y_test = X[800:], y[800:] # 20% of the data for the test set

# create non-linearity with more hidden layers

# set the random seed
tf.random.set_seed(42)

# 1. create the model

model = tf.keras.models.Sequential([
  tf.keras.layers.Dense(100, activation=tf.keras.activations.relu) ,
  tf.keras.layers.Dense(100, activation=tf.keras.activations.relu) ,                                                                 
  tf.keras.layers.Dense(1, activation=tf.keras.activations.sigmoid)                                  
])

# 2. compile the model

model.compile(loss='binary_crossentropy',
              optimizer=tf.keras.optimizers.Adam(lr=0.001),
              metrics=['binary_crossentropy'])

# 3. fit the moddel
def scheduler(epoch, lr):
   if epoch < 10:
     return lr
   else:
     return lr * tf.math.exp(-0.1)


history = model.fit(X_train,y_train, epochs=100)

from sklearn.metrics import confusion_matrix
y_preds = model.predict(X_test)

y_preds[:5]

confusion_matrix(y_test, tf.round(y_preds))

"""## confusion matrix"""

# Note: The following confusion matrix code is a remix of Scikit-Learn's 
# plot_confusion_matrix function - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.plot_confusion_matrix.html
# and Made with ML's introductory notebook - https://github.com/GokuMohandas/MadeWithML/blob/main/notebooks/08_Neural_Networks.ipynb 
import itertools

figsize = (10, 10)

# Create the confusion matrix
cm = confusion_matrix(y_test, tf.round(y_preds))
cm_norm = cm.astype("float") / cm.sum(axis=1)[:, np.newaxis] # normalize it
n_classes = cm.shape[0]

# Let's prettify it
fig, ax = plt.subplots(figsize=figsize)
# Create a matrix plot
cax = ax.matshow(cm, cmap=plt.cm.Blues) # https://matplotlib.org/3.2.0/api/_as_gen/matplotlib.axes.Axes.matshow.html
fig.colorbar(cax)

# Create classes
classes = False

if classes:
  labels = classes
else:
  labels = np.arange(cm.shape[0])

# Label the axes
ax.set(title="Confusion Matrix",
       xlabel="Predicted label",
       ylabel="True label",
       xticks=np.arange(n_classes),
       yticks=np.arange(n_classes),
       xticklabels=labels,
       yticklabels=labels)

# Set x-axis labels to bottom
ax.xaxis.set_label_position("bottom")
ax.xaxis.tick_bottom()

# Adjust label size
ax.xaxis.label.set_size(20)
ax.yaxis.label.set_size(20)
ax.title.set_size(20)

# Set threshold for different colors
threshold = (cm.max() + cm.min()) / 2.

# Plot the text on each cell
for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
  plt.text(j, i, f"{cm[i, j]} ({cm_norm[i, j]*100:.1f}%)",
           horizontalalignment="center",
           color="white" if cm[i, j] > threshold else "black",
           size=15)

"""# Multi-class classficiation

when you have more than 2 classes to predict, it is known as a **multi-class classification** task.
"""

import tensorflow as tf
from tensorflow.keras.datasets import fashion_mnist
import matplotlib.pyplot as plt

(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()
assert x_train.shape == (60000, 28, 28)
assert x_test.shape == (10000, 28, 28)
assert y_train.shape == (60000,)
assert y_test.shape == (10000,)

tf.unique(y_train)

class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 
               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']

# How many classes are there (this'll be our output shape)?
len(class_names)

# Plot the sample along with the label
sample_img_index = 14
print("Target label: {}".format(class_names[y_train[sample_img_index]]))
plt.imshow(x_train[sample_img_index])

"""# Modeling

A multi-class classification task

A vanilla model built as the baseline
"""

tf.random.set_seed(42)

tf.keras.backend.clear_session()

model_multi_class = tf.keras.models.Sequential([
 tf.keras.layers.Flatten(input_shape=(28,28)),
 tf.keras.layers.Dense(100, activation='relu'),
#  there are 10 classes in the dataset
 tf.keras.layers.Dense(10, activation='softmax')
])

model_multi_class.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              optimizer=tf.keras.optimizers.Adam(),
              metrics='accuracy')

history = model_multi_class.fit(x_train, y_train, epochs=10)

model_multi_class.summary()

model_multi_class.evaluate(x_test, y_test)

"""## Add a normalization

As neural nets tend to perform better with normalized data
"""

tf.random.set_seed(42)

tf.keras.backend.clear_session()

model_multi_class_norm = tf.keras.models.Sequential([
 tf.keras.layers.Flatten(input_shape=(28,28)),
 tf.keras.layers.BatchNormalization(),
 tf.keras.layers.Dense(100, activation='relu'),
#  there are 10 classes in the dataset
 tf.keras.layers.Dense(10, activation='softmax')
])

model_multi_class_norm.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              optimizer=tf.keras.optimizers.Adam(),
              metrics='accuracy')

history_norm = model_multi_class_norm.fit(x_train, y_train, epochs=10)

model_multi_class_norm.evaluate(x_test, y_test)

"""## Comparison of the two types of model """

import pandas as pd

print("non-norm model training log")
pd.DataFrame(history.history).plot()

print("Norm model training log")
pd.DataFrame(history_norm.history).plot()

"""## Finding the best learning rate"""

tf.random.set_seed(42)

tf.keras.backend.clear_session()

model_lr = tf.keras.models.Sequential([
 tf.keras.layers.Flatten(input_shape=(28,28)),
 tf.keras.layers.BatchNormalization(),
 tf.keras.layers.Dense(100, activation='relu'),
#  there are 10 classes in the dataset
 tf.keras.layers.Dense(10, activation='softmax')
])

model_lr.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              optimizer=tf.keras.optimizers.Adam(),
              metrics='accuracy')


history_lr = model_lr.fit(x_train, y_train, epochs=40, callbacks=[tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-4 * 10 **(epoch/20))])

model_lr.evaluate(x_test, y_test)

df_lr = pd.DataFrame(history_lr.history)
df_lr.head()

import numpy as np

lr_value = 1e-4 *10**np.arange(40)/20
plt.semilogx(df_lr["lr"], df_lr["loss"])
plt.title("Loss by different learning rate")
plt.xlabel("learning rate (log)")
plt.ylabel("Loss")

"""## Fit the model with the optimal learning rate"""

tf.random.set_seed(42)

tf.keras.backend.clear_session()

model_best_lr = tf.keras.models.Sequential([
 tf.keras.layers.Flatten(input_shape=(28,28)),
 tf.keras.layers.BatchNormalization(),
 tf.keras.layers.Dense(100, activation='relu'),
#  there are 10 classes in the dataset
 tf.keras.layers.Dense(10, activation='softmax')
])

model_best_lr.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              optimizer=tf.keras.optimizers.Adam(lr=1e-3),
              metrics='accuracy')

history_best_lr = model_best_lr.fit(x_train, y_train, epochs=40)

model_best_lr.evaluate(x_test, y_test)

y_preds = model_best_lr.predict(x_test)

y_preds_label = [tf.argmax(i) for i in y_preds]

len(y_preds_label)

y_preds = y_preds.argmax(axis=1)

y_test.shape

"""### Visualize the classification confusion matrix"""

# Note: The following confusion matrix code is a remix of Scikit-Learn's 
# plot_confusion_matrix function - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.plot_confusion_matrix.html
# and Made with ML's introductory notebook - https://github.com/GokuMohandas/MadeWithML/blob/main/notebooks/08_Neural_Networks.ipynb 
import itertools
from sklearn.metrics import confusion_matrix

figsize = (10, 10)


def make_confusion_matrix(y_preds, y_test, fig_size):
    # Create the confusion matrix
    cm = confusion_matrix(y_test, tf.round(y_preds))
    cm_norm = cm.astype("float") / cm.sum(axis=1)[:, np.newaxis] # normalize it
    n_classes = cm.shape[0]

    # Let's prettify it
    fig, ax = plt.subplots(figsize=fig_size)
    # Create a matrix plot
    cax = ax.matshow(cm, cmap=plt.cm.Blues) # https://matplotlib.org/3.2.0/api/_as_gen/matplotlib.axes.Axes.matshow.html
    fig.colorbar(cax)

    # Create classes
    classes = False

    if classes:
      labels = classes
    else:
      labels = np.arange(cm.shape[0])

    # Label the axes
    ax.set(title="Confusion Matrix",
          xlabel="Predicted label",
          ylabel="True label",
          xticks=np.arange(n_classes),
          yticks=np.arange(n_classes),
          xticklabels=labels,
          yticklabels=labels)

    # Set x-axis labels to bottom
    ax.xaxis.set_label_position("bottom")
    ax.xaxis.tick_bottom()

    # Adjust label size
    ax.xaxis.label.set_size(20)
    ax.yaxis.label.set_size(20)
    ax.title.set_size(10)

    # Set threshold for different colors
    threshold = (cm.max() + cm.min()) / 2.

    # Plot the text on each cell
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
      plt.text(j, i, f"{cm[i, j]} ({cm_norm[i, j]*100:.1f}%)",
              horizontalalignment="center",
              color="white" if cm[i, j] > threshold else "black",
              size=15)

make_confusion_matrix(y_preds_label, y_test, fig_size=(20,20))

"""## See what pattern the model learns"""

model_best_lr.layers

weight, biases = model_best_lr.layers[2].get_weights()
weight, weight.shape

biases.shape

from tensorflow.keras.utils import plot_model
plot_model(model_best_lr, show_shapes=True)

