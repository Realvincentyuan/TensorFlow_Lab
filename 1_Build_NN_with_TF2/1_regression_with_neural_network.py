# -*- coding: utf-8 -*-
"""01_regression_with_neural_network.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/189rs14DI0nStKbGetxSxPPn8toAdq26K

# Regression with a neural net built with TensorFlow
"""

import tensorflow as tf
print(tf.__version__)

"""# Create data to view and fit"""

import numpy as np
import matplotlib.pyplot as plt

# Create features
X = np.array([-7.0, -4.0, -1.0, 2.0, 5.0, 8.0, 11.0, 14.0])

# Create labels
y = np.array([3.0, 6.0, 9.0, 12.0, 15.0, 18.0, 21.0, 24.0])

# Visualize it
plt.scatter(X, y);

y == X + 10

"""# Input and output shapes"""

# Create a tensor for the housing price prediction
y.shape

X.shape

# Turn array to tensors
X = tf.constant(X)
y = tf.constant(y)
X,y

"""## Step in modeling with TensorFlow

1. Create a model - define input, hidden and output layers
2. Compile a model - define the loss, optimizer
3. Fitting a model - find the patter of features and labels
"""

tf.expand_dims(X, axis=-1)

tf.keras.backend.clear_session()

# Set the seed
tf.random.set_seed(42)

# 1. create a model
model = tf.keras.Sequential([
  tf.keras.layers.Dense(1)
])

# 2. Compile a model
model.compile(loss='mae', optimizer='SGD')

# 3. Fit
model.fit(tf.expand_dims(X, axis=-1),y, epochs=5)

model.predict(tf.constant([5]))

"""# Improve the model

- More complex models
- More suitable loss, metrics and optimizers
- More examples to train
"""

tf.keras.backend.clear_session()

# Set the seed
tf.random.set_seed(42)

# 1. create a model
model = tf.keras.Sequential([
  tf.keras.layers.Dense(100, activation='relu'),
  tf.keras.layers.Dense(100, activation='relu'),                             
  tf.keras.layers.Dense(1)
])

# 2. Compile a model
model.compile(loss='mae', 
              optimizer=tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.9))

# 3. Fit
model.fit(tf.expand_dims(X, axis=-1),y, epochs=100)

"""# Evaluate a model

In practice, a typical workflow is that after fitting, it needs to be evaluated and tweaked again.
"""

# Make a bigger dataset
X = tf.range(-100, 100,4)
X

y = X + 10
y

# Visualize the data
plt.scatter(X,y)

"""### Three sets

- **training set**: typicall 70%~80% of the dataset is used for model training
- **validation set**: 10%~15% of the entire dataset has been used for model being tuned
- **test set**: the model gets evaluated on this set to see how the model performs literally in the out-of-time samples
"""

# A vanilla way to split data: first 80% for training, the rest for testing
training_ratio = int(0.8 * X.shape[0])

X_train = X[:training_ratio]
X_test = X[training_ratio:]

y_train = y[:training_ratio]
y_test = y[training_ratio:]

"""### Visualize the data"""

plt.figure(figsize=(10,7))
plt.scatter(X_train, y_train, c='b',label='Training data')
plt.scatter(X_test, y_test, c='r',label='Test data')

tf.keras.backend.clear_session()

# Set the seed
tf.random.set_seed(42)

# 1. create a model
model = tf.keras.Sequential([
  tf.keras.layers.Input(shape=[1]),
  tf.keras.layers.Dense(100, activation='relu'),
  tf.keras.layers.Dense(100, activation='relu'),                             
  tf.keras.layers.Dense(1)
])

# 2. Compile a model
model.compile(loss='mae', 
              optimizer=tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.9),
              metrics='mae')

model.summary()

"""**Note**

- Total params: total number of params for the model to learn.
- Trainable parameters: these are the parameters the model can update as it trains
- Non-trainable params: the params that cannot be updated during the model trains, typically these are the params brought from other models(such as pre-trained mdoels in **transfer learning**) and we want to leverage the pre-trained models 
"""

# 3. Fit
model.fit(tf.expand_dims(X_train, axis=-1),y_train, epochs=100)

model.evaluate(X_test, y_test)

from tensorflow.keras.utils import plot_model

plot_model(model=model, show_shapes=True)

"""### Visualize model's prediction"""

y_preds = model.predict(y_test)

plt.scatter(X_test,y_test,c='b',label='True values')
plt.scatter(X_test,y_preds,c='r',label='Predicted values')
plt.scatter(X_train, y_train, c='g',label='Training data')

plt.legend()

y_test

tf.squeeze(y_preds)

# remember to check the shape of the two inputs to ensure they are aligned
tf.keras.metrics.mean_absolute_error(y_test, tf.squeeze(y_preds))

tf.keras.metrics.mse(y_test, tf.squeeze(y_preds))

"""### Runing experiments to improve our model

1. Get more data
2. Make the model more complex
3. Train for longer with more epochs
"""

X_train, y_train

"""**Note:**

It is a great habit to track model's performance. 

Luckily, there are a few solutions built-in with TensorFlow:

- **TensorBoard**
- **Weight and biases** (plug-in right into TensorBoard)

# Save the model

Two main formats:
- The SavedModel format
- The HDF5 format
"""

# Use the SavedModel format
model.save('./saved_model')

!ls -l /content/saved_model

# Save the model using the HDF5 format(better applicable in a universal environment)
model.save('./model.h5')

"""# Load the model

## Load a SavedModel
"""

restored_model = tf.keras.models.load_model("/content/saved_model")

restored_model.summary()

y_preds_2 = restored_model.predict(X_test)
y_preds = model.predict(X_test)
y_preds_2 == y_preds

"""## Load a model.h5"""

restored_model_from_h5 = tf.keras.models.load_model("model.h5")
restored_model_from_h5.summary()

y_preds_3 = restored_model_from_h5.predict(X_test)
y_preds = model.predict(X_test)
y_preds_3 == y_preds

"""# Download files from Colab

1. Use the files tab on the left and click `download`
2. Use code as below
"""

from google.colab import files
files.download("model.h5")

"""# A larger data"""

import tensorflow as tf
import pandas as pd
import matplotlib.pyplot as plt

# Read in the insurance dataset
insurance = pd.read_csv("https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/master/insurance.csv")

insurance.head()

insurance.info()

insurance_one_hot = pd.get_dummies(insurance)
insurance_one_hot.head()

# Create X & y for training
X = insurance_one_hot.drop("charges",axis=1)
y = insurance_one_hot["charges"]

X.head()

y[:5]

X.shape

# Create training and test sets
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

tf.random.set_seed(42)
tf.keras.backend.clear_session()

# 1. Create a model
insurance_model = tf.keras.Sequential([
     tf.keras.layers.Input(shape=11),                                   
      tf.keras.layers.Dense(4, activation='relu'),
      tf.keras.layers.Dense(8, activation='relu'),
      tf.keras.layers.Dense(16, activation='relu'),
      tf.keras.layers.Dense(32, activation='relu'),
      tf.keras.layers.Dense(64, activation='relu'),
      tf.keras.layers.Dense(128, activation='relu'),
      tf.keras.layers.Dense(256, activation='relu'),
      tf.keras.layers.Dense(1)

])

# 2. Compile the model
insurance_model.compile(loss='mae', 
                        optimizer=tf.keras.optimizers.Adam(),
                        metrics='mae')

insurance_model.summary()

epochs = 200

early_stopping = tf.keras.callbacks.EarlyStopping(patience=10)

history = insurance_model.fit(X_train, y_train, epochs=epochs, callbacks=[early_stopping])

history.params

history.history.keys()

# visualize model results

acc = history.history['mae']

loss = history.history['loss']

epochs_range = range(200)

plt.figure(figsize=(16, 8))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training MAE')
plt.legend(loc='lower right')
plt.title('Training and Validation MAE')

insurance_model.evaluate(X_test, y_test)

"""## Preprocessing data

- Normalization
- Standardization

Neural nets tend to work better with normalization, in fact.
"""

insurance.head()

from sklearn.preprocessing import MinMaxScaler, OneHotEncoder
from sklearn.compose import make_column_transformer

# Create a column transformer
ct = make_column_transformer(
    (MinMaxScaler(),['age','bmi','children']),
    (OneHotEncoder(handle_unknown="ignore"),["sex",'smoker','region'])
                             )

from sklearn.model_selection import train_test_split

# Create X & y
X = insurance.drop('charges', axis=1)
y = insurance['charges']

# Build the training and test dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Fit the columns transformers to our dataset
ct.fit(X_train)

# Transfrom the dataset with normalization and OneHotEncoder
X_train_normal = ct.transform(X_train)
X_test_normal = ct.transform(X_test)

X_train.loc[0]

X_train_normal[0]

tf.random.set_seed(42)
tf.keras.backend.clear_session()

# 1. Create a model
insurance_model = tf.keras.Sequential([
     tf.keras.layers.Input(shape=11),                                   
      tf.keras.layers.Dense(4, activation='relu'),
      tf.keras.layers.Dense(8, activation='relu'),
      tf.keras.layers.Dense(16, activation='relu'),
      tf.keras.layers.Dense(32, activation='relu'),
      tf.keras.layers.Dense(64, activation='relu'),
      tf.keras.layers.Dense(128, activation='relu'),
      tf.keras.layers.Dense(256, activation='relu'),
      tf.keras.layers.Dense(1)

])

# 2. Compile the model
insurance_model.compile(loss='mae', 
                        optimizer=tf.keras.optimizers.Adam(),
                        metrics='mae')

insurance_model.summary()

epochs = 200

early_stopping = tf.keras.callbacks.EarlyStopping(patience=10)

history = insurance_model.fit(X_train_normal, y_train, epochs=epochs, callbacks=[early_stopping])

insurance_model.evaluate(X_test_normal, y_test)

