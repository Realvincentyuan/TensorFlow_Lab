# -*- coding: utf-8 -*-
"""00_tensorflow_basics.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1khCexG8N6AkKonWykasKwYbsfG1JaFrR

# Dependencies
"""

import tensorflow as tf

"""# Make a tensor with `tf.constant()`"""

tensor_1 = tf.constant([1.,2.])

"""# Create a tensor with `tf.Variable()`"""

unchangable_tensor = tf.constant([10, 7])
changable_tensor = tf.Variable([10, 7])

unchangable_tensor

changable_tensor

changable_tensor[0].assign(15)

"""# Create a random tensor """

random_tensor = tf.random.uniform([3,3])

random_tensor

random_tensor.ndim

"""# Shuffle a tensor"""

not_shuffled = tf.constant([[10,8],[9,7],[8,5]])

not_shuffled

# this will still be random if a global random seed is not set
tf.random.shuffle(not_shuffled, seed=42)

tf.random.set_seed(42)
tf.random.shuffle(not_shuffled, seed=42)

"""# Other ways to create a tensor  """

tf.ones([10,2])

tf.zeros([2,3])

# turn numpy array into TF tensor, tensor can be run on GPUs with much faster speed
import numpy as np
numpy_A = np.arange(1,25)

numpy_A

A = tf.constant(numpy_A, shape=(2,3,4))
A

A.ndim

B = tf.constant(numpy_A, shape=(3,8))
B

"""# Get information from tensors

- Shape
- Rank
- Axis or dimension
- Size
"""

# Create a rank 4 tensor

rank_4_tensor = tf.ones([2,3,4,5])
rank_4_tensor

rank_4_tensor.shape, rank_4_tensor.ndim, tf.size(rank_4_tensor)

# Get various attributes or the tensor
print("Data type: ", rank_4_tensor.dtype)
print("Number of dimension(rank):",rank_4_tensor.ndim)
print("Shape of tensor: ", rank_4_tensor.shape)
print("Elements along the 0 axis:", rank_4_tensor.shape[0])
print("Elements along the last axis:", rank_4_tensor.shape[-1])
print("Total number of elements:",  tf.size(rank_4_tensor).numpy())

"""# Indexing tensors"""

# Get the first two elements
rank_4_tensor[:2, :2,:2,:2]

# Get first element from each dimention from each index except for the final one
rank_4_tensor[:1, :1,:1]

rank_4_tensor[:1, :1,:, :1]

# Create a rank 2 dimension tensor
rank_2_tensor = tf.constant([[10,7],[4,5]])

rank_2_tensor

rank_2_tensor.shape, rank_2_tensor.ndim

rank_2_tensor[:,-1]

# Add an extra dimension to our rank 2 tensor
rank_3_tensor = rank_2_tensor[..., tf.newaxis]
rank_3_tensor

# Alternative of tf.newaxis
tf.expand_dims(rank_2_tensor, axis=-1)

# Alternative of tf.newaxis
tf.expand_dims(rank_2_tensor, axis=0)

"""# Manipulating tensors

- Addition
- Minus
- Multiplication
- Devide
"""

tensor = tf.constant([[10,7],[4,5]])
tensor + 10

# Manipulation
tensor * 10

tensor - 10

tensor / 10

# Use tensorflow built-in functions, which is more friendly with GPU 
tf.multiply(tensor, 10)

"""# Matrix Multiplication (dot product)

Prerequisite:
- The inner dimension must match
- the resulting dimension equals to the outer dimension

**Generally, you want to transpose tensor instead of reshaping them if the tensors are not aligned with shapes**
"""

print(tensor)

tf.matmul(tensor, tensor)

tensor * tensor

# Python Operator for matrix Multiplication
tensor @ tensor

X = tf.constant([[3,2],[2,4],[2,3]])
Y = tf.constant([[6,7],[3,7],[4,8]])

tf.matmul(X, tf.transpose(Y))

tf.matmul(X, tf.reshape(Y, shape=(2,3)))

tf.matmul(X, tf.transpose(Y))

tf.tensordot(X, tf.transpose(Y), axes=1)

# Dive deep into why tf.transpose and tf.reshpe is different
print("Original value of Y:", Y)
print('\n')
print("Tensor reshape, with shape=(2,3): ", tf.reshape(Y, shape=(2,3)))
print('\n')
print("Tensor transpose: ", tf.transpose(Y))

"""# Change the datatype of a tensor"""

B = tf.constant([1,7, 3.4])
B.dtype

C = tf.constant([7, 4])
C.dtype

#  Change dtypes to reduce precision (to float16)
B = tf.cast(B, dtype=tf.float16)
B

"""# Tensor aggregation"""

# Get the absolute value
D = tf.constant([-7,-10])
D

tf.abs(D)

"""Type of aggregation below:

- Minimum of the tensor
- Maximum of the tensor
- Mean of the tensor
- Sum of the tensor
"""

# Create a random tensor with values between 0 and 100 of size 50
E = tf.constant(np.random.randint(0,100, size=50))
E

# Find the min
tf.reduce_min(E)

# Find the max
tf.reduce_max(E)

# Find the mean
tf.reduce_mean(E)

# Find the sum
tf.reduce_sum(E)

E = tf.cast(E, dtype=tf.float16)

tf.math.reduce_variance(E)

tf.math.reduce_std(E)

"""# Find the postional min and max"""

tf.argmax(E)

tf.argmin(E)

"""# Squeeze tensors (removing all single dimensions)"""

G = tf.constant(tf.random.uniform(shape=[50]), shape=(1,1,1,1,50))
G

G.shape

G_squeeze = tf.squeeze(G)
G_squeeze, G_squeeze.shape

"""# One-hot encoding tensors"""

# create a list of indices
some_list =[0,1,2,3] #could be red, green, blue, yellow

tensor_one_hot = tf.one_hot(some_list, depth=4)
tensor_one_hot

# Specify custom values for one hot encodings
tf.one_hot(some_list, depth=4, on_value="This is an example.", off_value="Hello, world.")

"""# Square, log and square root"""

H = tf.range(10)

tf.square(H)

tf.sqrt(tf.cast(H, dtype=tf.float16))

tf.math.log(tf.cast(H, dtype=tf.float16))

"""# Tensor and Numpy"""

# Create a tensor directly from a numpy array
J = tf.constant(np.array([1,2,3,4]))
J

# Convert the tensor back to a numpy array
np.array(J), type(np.array(J))

numpy_J = tf.constant(np.array([1,2,3,4]))
tensor_J =  tf.constant([1,2,3,4])

numpy_J.dtype, tensor_J.dtype

"""# Tensors working better with GPUs"""

import tensorflow as tf

tf.config.list_physical_devices()

!nvidia-smi

"""ðŸ”‘ **Note:** If a CUDA enabled GPU is acceesible, tensorflow will use it as much as possible"""

