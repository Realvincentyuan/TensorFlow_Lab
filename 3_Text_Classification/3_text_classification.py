# -*- coding: utf-8 -*-
"""TF v2.8 classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PsbT2qE661fFl11OQwG2EsxytBv5V25j

# Objectives

- Go through the end-to-end process of text classification from text loading to modeling
- Update the workflow with the up-to-date TensorFlow API including but not limited to tf.data, tf.keras etc.

# Dependencies
"""

!pip install "tensorflow-text==2.8.*"


import tensorflow as tf
from tensorflow.keras import layers
from tensorflow.keras import losses
from tensorflow.keras import utils
from tensorflow.keras.layers import TextVectorization

import tensorflow_datasets as tfds
import tensorflow_text as tf_text

import collections

import plotly.express as px
import matplotlib.pyplot as plt

"""# Load dataset"""

BATCH_SIZE = 32

# Training set.
train_ds = tfds.load(
    'imdb_reviews',
    split='train[:80%]',
    # batch_size=BATCH_SIZE,
    shuffle_files=True,
    as_supervised=True)

# Validation set.
val_ds = tfds.load(
    'imdb_reviews',
    split='train[80%:]',
    # batch_size=BATCH_SIZE,
    shuffle_files=True,
    as_supervised=True)

print(train_ds.cardinality().numpy())
print(val_ds.cardinality().numpy())

for data, label in  train_ds.take(1):
  print(type(data))
  print('Text:', data.numpy())
  print('Label:', label.numpy())
  break

"""# Preprocessing

## Convert text to lower case
"""

train_ds = train_ds.map(lambda text, label: (tf_text.case_fold_utf8(text), label))

for data, label in  train_ds.take(1):
  print(type(data))
  print('Text:', data)
  print('Label:', label.numpy())
  break

"""## Formattting texts with regex"""

str_regex_pattern = [("[^A-Za-z0-9(),!?\'\`]", " "),("\'s", " \'s",) ,("\'ve", " \'ve"),("n\'t", " n\'t"),("\'re", " \'re"),("\'d", " \'d")
,("\'ll", " \'ll"),(",", " , "),("!", " ! "),("\(", " \( "),("\)", " \) "),("\?", " \? "),("\s{2,}", " ")]

for pattern, rewrite in str_regex_pattern:
  train_ds = train_ds.map(lambda text, label: (tf.strings.regex_replace(text, pattern=pattern, rewrite=rewrite), label))

"""## Prepare a vocabulary"""

# Do not use validation set as that will lead to data leak
train_text = train_ds.map(lambda text, label: text)

tokenizer = tf_text.WhitespaceTokenizer()

unique_tokens = collections.defaultdict(lambda: 0)
for text in train_text.as_numpy_iterator():
  tokens = tokenizer.tokenize(text).numpy()
  for token in tokens:
    unique_tokens[token] += 1

sentence_length =  []
for text in train_text.as_numpy_iterator():
  tokens = tokenizer.tokenize(text).numpy()
  sentence_length.append(len(tokens))

# check out the average sentence length
sum(sentence_length)/len(sentence_length)

d_view = [ (v,k) for k,v in unique_tokens.items()]
d_view.sort(reverse=True) # natively sort tuples by first element
for v,k in d_view[:10]:
    print("%s: %d" % (k,v))

len(d_view)

d_view[20000]

"""## Select the appropriate vocab size"""

fig = px.scatter(x=range(len(d_view)), y=[cnt for cnt, word in d_view])
fig.show()

vocab_size = 20000

keys = [token for cnt, token in d_view][:vocab_size]
values = range(2, len(keys) + 2)  # Reserve `0` for padding, `1` for OOV tokens.

keys[:5]

len(values)

"""## Create a string to index lookup

**ref**: 
- [tf.lookup.KeyValueTensorInitializer](https://www.tensorflow.org/api_docs/python/tf/lookup/KeyValueTensorInitializer#args)
- [tf.lookup.StaticVocabularyTable](https://www.tensorflow.org/api_docs/python/tf/lookup/StaticVocabularyTable#args)
"""

num_oov_buckets =1

# Note: must assign the key_dtype and value_dtype when the keys and values are Python arrays
init = tf.lookup.KeyValueTensorInitializer(
    keys= keys,
    values= values,
    key_dtype=tf.string, value_dtype=tf.int64)

table = tf.lookup.StaticVocabularyTable(
   init,
   num_oov_buckets=num_oov_buckets)

# Test the look up table with sample input
input_tensor = tf.constant(["emerson", "lake", "palmer", "king"])
table[input_tensor].numpy()

"""## Convert text into index"""

def text_index_lookup(text, label):
  tokenized = tokenizer.tokenize(text)
  vectorized = table.lookup(tokenized)
  return vectorized, label

train_ds = train_ds.map(text_index_lookup)
val_ds = val_ds.map(text_index_lookup)

for token, label in train_ds.take(1):
  print('Token', token)
  print('Label', label)

"""## Configure dataset for performance"""

AUTOTUNE = tf.data.AUTOTUNE

def configure_dataset(dataset):
  return dataset.cache().prefetch(buffer_size=AUTOTUNE)

train_ds = configure_dataset(train_ds)
val_ds = configure_dataset(val_ds)

sample_text, sample_labels = next(iter(val_ds))
print("Text batch shape: ", sample_text.shape)
print("Label batch shape: ", sample_labels.shape)
print("First text example: ", sample_text[0])
print("First label example: ", sample_labels[0])

train_ds = train_ds.padded_batch(BATCH_SIZE  )
val_ds = val_ds.padded_batch(BATCH_SIZE  )

for token, label in train_ds.take(1):
  print('Token', token[2].shape)
  print('Lablel', label[1])

"""# Download and process test set"""

# Test set.
test_ds = tfds.load(
    'imdb_reviews',
    split='test',
    # batch_size=BATCH_SIZE,
    shuffle_files=True,
    as_supervised=True)

test_ds = test_ds.map(lambda text, label: (tf_text.case_fold_utf8(text), label))

for pattern, rewrite in str_regex_pattern:
  test_ds = test_ds.map(lambda text, label: (tf.strings.regex_replace(text, pattern=pattern, rewrite=rewrite), label))

test_ds = test_ds.map(text_index_lookup)
test_ds = configure_dataset(test_ds)
test_ds = test_ds.padded_batch(BATCH_SIZE  )

"""# Modeling - CNN

with sequential API
"""

vocab_size += 2 # 0 for padding and 1 for oov token

vocab_size

"""## Create models"""

def create_model(vocab_size, num_labels, dropout_rate):
  model = tf.keras.Sequential([
      tf.keras.layers.Embedding(vocab_size, 128, mask_zero=True),

      tf.keras.layers.Conv1D(32, 3, padding="valid", activation="relu", strides=1),
      tf.keras.layers.MaxPooling1D(pool_size=2),

      tf.keras.layers.Conv1D(64, 4, padding="valid", activation="relu", strides=1),
      tf.keras.layers.MaxPooling1D(pool_size=2),

      tf.keras.layers.Conv1D(128, 5, padding="valid", activation="relu", strides=1),
      tf.keras.layers.GlobalMaxPooling1D( ),

      tf.keras.layers.Dropout(dropout_rate),

      tf.keras.layers.Dense(num_labels)
  ])
  return model

tf.keras.backend.clear_session()
model = create_model(vocab_size=vocab_size, num_labels=2, dropout_rate=0.5)

loss = losses.SparseCategoricalCrossentropy(from_logits=True)
optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)


model.compile(loss=loss, optimizer=optimizer, metrics='accuracy')

model.summary()

"""## Fitting"""

early_stopping = tf.keras.callbacks.EarlyStopping(patience=10)
epochs = 100
history = model.fit(x=train_ds, validation_data=val_ds,epochs=epochs, callbacks=[early_stopping])

"""## Evaluation on the test set"""

loss, accuracy = model.evaluate(test_ds)

print("Loss: ", loss)
print("Accuracy: {:2.2%}".format(accuracy))

"""## Performance"""

# visualize model results

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss = history.history['loss']
val_loss = history.history['val_loss']

epochs_range = range(24)

plt.figure(figsize=(16, 8))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()

"""# Modeling - LSTM

Using functional API
"""

dropout_rate = 0.5
num_labels = 2

input = tf.keras.layers.Input([None] )
x = tf.keras.layers.Embedding(
        input_dim=vocab_size,
        output_dim=128,
        # Use masking to handle the variable sequence lengths
        mask_zero=True)(input)

x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64))(x)
x = tf.keras.layers.Dense(64, activation='relu')(x)
x = tf.keras.layers.Dropout(dropout_rate)(x)
output = tf.keras.layers.Dense(num_labels)(x)

lstm_model = tf.keras.Model(inputs=input, outputs=output, name="text_lstm_model")

lstm_model.summary()

loss = losses.SparseCategoricalCrossentropy(from_logits=True)
optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)

lstm_model.compile(loss=loss, optimizer=optimizer, metrics='accuracy')

early_stopping = tf.keras.callbacks.EarlyStopping(patience=10)
epochs = 100
history_2 = lstm_model.fit(x=train_ds, validation_data=val_ds,epochs=epochs, callbacks=[early_stopping])

# visualize model results

acc = history_2.history['accuracy']
val_acc = history_2.history['val_accuracy']

loss = history_2.history['loss']
val_loss = history_2.history['val_loss']

epochs_range = range(24)

plt.figure(figsize=(16, 8))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()

loss, accuracy = lstm_model.evaluate(test_ds)

print("Loss: ", loss)
print("Accuracy: {:2.2%}".format(accuracy))