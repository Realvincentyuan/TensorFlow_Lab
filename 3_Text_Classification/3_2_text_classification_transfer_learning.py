# -*- coding: utf-8 -*-
"""08_NLP_with_TensorFlow.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-tor_8C0vNXGN6_IHAwgg-l_oaVqR_nm
"""

# Download helper functions script
!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py

# Import series of helper functions for the notebook
from helper_functions import unzip_data, create_tensorboard_callback, plot_loss_curves, compare_historys

# Download disaster data (same as from Kaggle)
!wget "https://storage.googleapis.com/ztm_tf_course/nlp_getting_started.zip"

# Unzip data
unzip_data("nlp_getting_started.zip")

# Turn .csv files into pandas DataFrame's
import pandas as pd
train_df = pd.read_csv("train.csv")
test_df = pd.read_csv("test.csv")
train_df.head()

train_df_shuffled = train_df.sample(frac=1)
train_df_shuffled.head()

# The labels are relatively balanced
train_df_shuffled["target"].value_counts()

# Total samples
train_df.shape, test_df.shape

test_df.head()

import random
random_index = random.randint(0, len(train_df))

print(f"Random index: {random_index}")
# print some random samples
train_df_shuffled[["text","target"]][random_index:random_index+5]

"""## Split the data into training and validation dataset"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    train_df["text"], train_df["target"], test_size=0.1,
     random_state=42)

len(X_train), len(y_train), len(X_test), len(y_test)

# Check some samples
X_train.iloc[:5], y_train.iloc[:5]

"""## Convert text into numbers - Vectorization"""

import tensorflow as tf
from tensorflow.keras.layers import TextVectorization

# Some default values for the TextVectorization
text_vectorizar = TextVectorization(max_tokens=None,
                                    standardize="lower_and_strip_punctuation",
                                    split="whitespace",
                                    output_mode="int",
                                    output_sequence_length=None
                                    )

import numpy as np
# Check the sequence length of input data
np.mean(X_train.apply( lambda x: len(x.split())))

np.max(X_train.apply( lambda x: len(x.split())))

np.median(X_train.apply( lambda x: len(x.split())))

max_vocab_length = 10000 # max number of words to have in out vocab
max_length = 15 

# Some default values for the TextVectorization
text_vectorizar = TextVectorization(max_tokens=max_vocab_length,
                                    standardize="lower_and_strip_punctuation",
                                    split="whitespace",
                                    output_mode="int",
                                    output_sequence_length=max_length
                                    )

# Fit the text vectorizer to the input text
text_vectorizar.adapt(X_train)

sample_text = "There is a flood in the downtown"

text_vectorizar.apply([sample_text])

# Test the vectorizer on the input sample sample
print(f"Random index: {random_index}")
print(f"\nOriginal text: {X_train[random_index]}")
print(f"\nVecoterized text: {text_vectorizar([X_train[random_index]])}")

# Get the unique words in the vocab
words_in_vocab = text_vectorizar.get_vocabulary()
top_5_words = words_in_vocab[:5]
print(f"Number of words: {len(words_in_vocab)}")
print(f"Top 5 words: {top_5_words}")

"""# Create an embedding layer

Most of the important params are:

- **Input_dim**: size of our vocab
- **Output_dim**: the size of the output embedding
- **Input_length**: lenght of sequences passed into the embedding layer
"""

from tensorflow.keras import layers

embedding = layers.Embedding(input_dim=max_vocab_length,
                             output_dim=128,
                             input_length=max_length)

print(f"Original input text: {X_train[random_index]}")
print(f"\nOutput of the embedding layers: {embedding(text_vectorizar([X_train[random_index]]))}")

"""# Model 0: base model with TF-IDF and Multinomial Naive Bayes algorithm"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline

# Create tokenization and modelling pipeline
model_0 = Pipeline([
                    ("tfidf", TfidfVectorizer()), # convert words to numbers using tfidf
                    ("clf", MultinomialNB()) # model the text
])

# Fit the pipeline to the training data
model_0.fit(X_train, y_train)

base_line_socre = model_0.score(X_test, y_test)
base_line_socre

def calculate_results(y_true, y_pred):
  # Precision metric
  precision = tf.keras.metrics.Precision()
  precision.update_state(y_true, y_pred)
  precision = precision.result().numpy()

  # Recall
  recall = tf.keras.metrics.Recall()
  recall.update_state(y_true, y_pred)
  recall = recall.result().numpy()

  # F1 score
  f1_score = (2 * precision * recall)/(precision + recall)

  # Accuracy
  accuracy = tf.keras.metrics.Accuracy()
  accuracy.update_state(y_true, y_pred)
  accuracy = accuracy.result().numpy()

  model_results = {"accuracy": accuracy,
                  "precision": precision,
                  "recall": recall,
                  "f1": f1_score}
  return model_results

y_pred = model_0.predict(X_test)
calculate_results(y_test,y_pred)

"""# Model 1: dense deep learning model"""

from helper_functions import create_tensorboard_callback
from tensorflow.keras import layers

SAVE_DIR = "model_logs"

tf.random.set_seed(42)
tf.keras.backend.clear_session()


inputs = layers.Input(shape=(1,), dtype=tf.string)
x = text_vectorizar(inputs)
x = embedding(x)
x = layers.GlobalAveragePooling1D()(x) # Condense the feature vector for each token to one vector, otherwise the shape is wrong
outputs = layers.Dense(1, activation="sigmoid")(x)

model_1 = tf.keras.models.Model(inputs, outputs, name="model_1_dense")

model_1.summary()

model_1.compile(loss="binary_crossentropy",
                optimizer=tf.keras.optimizers.Adam(),
                metrics="accuracy")

model_1.fit(x=X_train,
            y=y_train,
            epochs=10,
            validation_data=(X_test, y_test),
            callbacks=[create_tensorboard_callback(dir_name=SAVE_DIR,
                                                   experiment_name=model_1.name)])

model_1.evaluate(X_test, y_test)

model_1_pred = model_1.predict(X_test)

# Check the model performance
calculate_results(y_test, tf.round(model_1_pred))

"""## Visualize the learned embeddings"""

weights = model_1.get_layer("embedding").get_weights()[0]
vocab = text_vectorizar.get_vocabulary()

print(f"Shape of embedding: {weights.shape}")

"""Visulize the embedding using the TensorFlow projector"""

import io
out_v = io.open('vectors.tsv', 'w', encoding='utf-8')
out_m = io.open('metadata.tsv', 'w', encoding='utf-8')

for index, word in enumerate(vocab):
  if index == 0:
    continue  # skip 0, it's padding.
  vec = weights[index]
  out_v.write('\t'.join([str(x) for x in vec]) + "\n")
  out_m.write(word + "\n")
out_v.close()
out_m.close()

try:
  from google.colab import files
  files.download('vectors.tsv')
  files.download('metadata.tsv')
except Exception:
  pass

"""Next, just need to upload the data into [TensorFlow Projector](http://projector.tensorflow.org/) wiht the 

- vectors
- metadata

# Model 2: Simple RNN
"""

tf.random.set_seed(42)
tf.keras.backend.clear_session()


inputs = layers.Input(shape=(1,), dtype=tf.string)
x = text_vectorizar(inputs)
x = embedding(x)

x = layers.SimpleRNN(32)(x) # Condense the feature vector for each token to one vector, otherwise the shape is wrong
outputs = layers.Dense(1, activation="sigmoid")(x)

model_2 = tf.keras.models.Model(inputs, outputs, name="model_2_rnn")

model_2.summary()

model_2.compile(loss="binary_crossentropy",
                optimizer=tf.keras.optimizers.Adam(),
                metrics=["accuracy"])

model_2.fit(x=X_train,
            y=y_train,
            epochs=5,
            validation_data=(X_test, y_test),
            callbacks=[create_tensorboard_callback(dir_name=SAVE_DIR,
                                                   experiment_name=model_2.name)])

model_2.evaluate(X_test, y_test)

model_2_pred = model_2.predict(X_test)

calculate_results(y_test, tf.round(model_2_pred))

"""# Model 2.1: Biredictional RNN"""

tf.random.set_seed(42)
tf.keras.backend.clear_session()


inputs = layers.Input(shape=(1,), dtype=tf.string)
x = text_vectorizar(inputs)
x = embedding(x)

x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32))(x)
x = layers.Dropout(0.5)(x)
x = layers.Dense(32)(x) 
outputs = layers.Dense(1, activation="sigmoid")(x)

model_2_1 = tf.keras.models.Model(inputs, outputs, name="model_2_1_bidir_rnn")

model_2_1.summary()

model_2_1.compile(loss="binary_crossentropy",
                optimizer=tf.keras.optimizers.Adam(),
                metrics=["accuracy"])

model_2_1.fit(x=X_train,
            y=y_train,
            epochs=5,
            validation_data=(X_test, y_test),
            callbacks=[create_tensorboard_callback(dir_name=SAVE_DIR,
                                                   experiment_name=model_2_1.name)])

print("="*20)
print("Model Evaluation")
model_2_1.evaluate(X_test, y_test)

model_2_1_pred = model_2_1.predict(X_test)

calculate_results(y_test, tf.round(model_2_1_pred))

"""# Modle 3: CNN"""

tf.random.set_seed(42)
tf.keras.backend.clear_session()


inputs = layers.Input(shape=(1,), dtype=tf.string)
x = text_vectorizar(inputs)
x = embedding(x)

x = layers.Conv1D(filters=32, kernel_size=3, activation="swish" )(x)
x = layers.GlobalMaxPool1D()(x)
x = layers.Dropout(0.5)(x)
outputs = layers.Dense(1, activation="sigmoid")(x)

model_3 = tf.keras.models.Model(inputs, outputs, name="model_2_1_bidir_rnn")

model_3.summary()

model_3.compile(loss="binary_crossentropy",
                optimizer=tf.keras.optimizers.Adam(),
                metrics=["accuracy"])

model_3.fit(x=X_train,
            y=y_train,
            epochs=5,
            validation_data=(X_test, y_test),
            callbacks=[create_tensorboard_callback(dir_name=SAVE_DIR,
                                                   experiment_name=model_3.name)])


print("\n"+"="*50)
print("Model Evaluation")
model_3.evaluate(X_test, y_test)

model_3_pred = model_3.predict(X_test)

calculate_results(y_test, tf.round(model_3_pred))

"""# Model 4: using transfer learning"""

!pip install -q -U "tensorflow-text==2.8.*"

!pip install -q tf-models-official==2.7.0

import os
import shutil

import tensorflow as tf
import tensorflow_hub as hub
import tensorflow_text as text
from official.nlp import optimization  # to create AdamW optimizer

import matplotlib.pyplot as plt

tfhub_handle_encoder = "https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/2"
tfhub_handle_preprocess = "https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3"

print(f'BERT model selected           : {tfhub_handle_encoder}')
print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')

# Build the BERT model
def build_classifier_model():
  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')
  preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')

  encoder_inputs = preprocessing_layer(text_input)
  encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')
  outputs = encoder(encoder_inputs)

  net = outputs['pooled_output']
  net = tf.keras.layers.Dropout(0.1)(net)
  net = tf.keras.layers.Dense(1, activation='sigmoid', name='classifier')(net)
  return tf.keras.Model(text_input, net)

model_4 = build_classifier_model()

model_4.summary()

# Loss and optimizer
loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)
metrics = tf.metrics.BinaryAccuracy()

"""The set up of optimizer depends on the original paper of BERT:

ref: https://www.tensorflow.org/text/tutorials/classify_text_with_bert
"""

epochs = 5
steps_per_epoch = len(X_train)
num_train_steps = steps_per_epoch * epochs
num_warmup_steps = int(0.1*num_train_steps)

init_lr = 3e-5
optimizer = optimization.create_optimizer(init_lr=init_lr,
                                          num_train_steps=num_train_steps,
                                          num_warmup_steps=num_warmup_steps,
                                          optimizer_type='adamw')

model_4.compile(loss=loss,
                optimizer=optimizer,
                metrics=metrics)

model_4.fit(x=X_train,
            y=y_train,
            epochs=epochs,
            validation_data=(X_test, y_test),
            callbacks=[create_tensorboard_callback(dir_name=SAVE_DIR,
                                                   experiment_name="BERT")])

model_4.evaluate(X_test, y_test)

model_4_pred = model_4.predict(X_test)

calculate_results(y_test, tf.round(tf.math.sigmoid(model_4_pred)))

"""ðŸ”¥ By far, the BERT model performs the best, this is expected considering the model architecture, intuitively, with more epochs, it can achieve greater results.

# Prediction speed/score tradeoff
"""

import time

def pred_timer(model, samples):
  """
  Times how long a model takes to make predictions on samples
  """
  start_time = time.perf_counter()
  model.predict(samples)
  end_time = time.perf_counter()
  total_time = end_time - start_time
  time_per_pred = total_time / len(samples)
  return total_time, time_per_pred



